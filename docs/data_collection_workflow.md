# データ収集ワークフロー

## 概要

このドキュメントでは、日本籍米国株式アクティブ投信のパフォーマンス検証に必要なデータを公開情報から収集するための完全自動化ワークフローを定義します。

---

## データソースの特定

### 主要データソース

日本の投資信託データを取得できる公開情報源は以下の通りです。

| データソース | URL | 提供情報 | 制約事項 |
|------------|-----|---------|---------|
| **投資信託協会 投信総合検索ライブラリー** | https://toushin-lib.fwg.ne.jp/FdsWeb/ | ファンド基本情報、基準価額、純資産総額 | Webスクレイピング禁止 |
| **投資信託協会 統計データ** | https://www.toushin.or.jp/statistics/ | 集計統計、月次レポート | 個別ファンドデータなし |
| **モーニングスター** | https://www.morningstar.co.jp/ | ファンド詳細情報、パフォーマンス | 有料データあり |
| **Yahoo!ファイナンス** | https://finance.yahoo.co.jp/ | 基準価額、チャート | 投資信託の網羅性に制約 |
| **各運用会社の公式サイト** | 各社URL | 目論見書、月次レポート、運用報告書 | 手動収集が必要 |

### 制約と課題

**投資信託協会の制約:**
> ○ 自動的な手段による情報の取得（Webスクレイピング、ロボットによるデータ収集、等）を禁止します。

この制約により、投資信託協会の投信総合検索ライブラリーからの自動データ収集は**不可能**です。

**代替アプローチ:**

1. **手動データ収集**: 投信総合検索ライブラリーから手動でCSVダウンロード（可能な場合）
2. **運用会社サイトからの収集**: 各運用会社の公式サイトから月次レポートを収集
3. **有料データサービス**: モーニングスターやBloombergなどの有料サービスを利用
4. **公開統計データ**: 投資信託協会の統計データから集計情報を取得

---

## 推奨データ収集戦略

### アプローチ1: 手動収集 + 半自動処理（現実的）

**ステップ1: ファンドリストの作成**
1. 投信総合検索ライブラリーで以下の条件で検索:
   - 投資対象地域: 北米
   - 投資対象資産: 株式
   - インデックスファンド区分: すべて
   - 運用年数: 3年以上
2. 検索結果を手動でCSV形式でエクスポート（可能な場合）
3. または、検索結果画面から手動でファンド名とISINコードをコピー

**ステップ2: 各ファンドの詳細情報収集**
1. 各ファンドの目論見書PDFをダウンロード
2. PDFから以下の情報を抽出:
   - 設定日
   - 信託報酬
   - ベンチマーク
   - 為替ヘッジの有無
   - 運用方針（アクティブ/パッシブの判定）

**ステップ3: 月次リターンデータの収集**
1. 各運用会社の月次レポートから基準価額を取得
2. または、Yahoo!ファイナンスから基準価額の時系列データを取得
3. 月次リターンを計算: `(今月末基準価額 / 前月末基準価額) - 1`

**ステップ4: データの統合とクレンジング**
1. 収集したデータを`fund_attributes.csv`と`monthly_returns.csv`に統合
2. データ品質チェックを実行
3. 36か月要件を満たさないファンドを除外

### アプローチ2: サンプルデータでのプロトタイプ（即座に実行可能）

**目的**: フレームワークの動作確認と分析手法の検証

**ステップ1: 代表的なファンドの選定**
- アクティブファンド: 10～20本（ヘッジあり/なし各5～10本）
- パッシブファンド: 3～5本（S&P500連動、ヘッジあり/なし）

**ステップ2: 手動データ入力**
- 選定したファンドの基本情報を手動で入力
- 月次基準価額を手動で入力（直近36か月分）

**ステップ3: 分析実行**
- サンプルデータで全分析を実行
- 出力フォーマットと分析ロジックを検証

**ステップ4: 本番データへの拡張**
- サンプルで検証後、本番データを収集

---

## 自動化可能な部分

以下の処理は自動化が可能です。

### 1. Yahoo!ファイナンスからの基準価額取得

**対象**: 一部の投資信託（Yahoo!ファイナンスに掲載されているもの）

**方法**:
```python
import yfinance as yf
import pandas as pd

# 投資信託のティッカーシンボル（Yahoo!ファイナンス形式）
# 例: eMAXIS Slim 米国株式(S&P500) → 03311187.T
ticker = "03311187.T"

# 基準価額の取得
fund = yf.Ticker(ticker)
hist = fund.history(period="3y", interval="1mo")

# 月次リターンの計算
monthly_returns = hist['Close'].pct_change()
```

**制約**: 
- すべての投資信託がYahoo!ファイナンスに掲載されているわけではない
- ティッカーシンボルの特定が必要

### 2. PDFからのテキスト抽出

**対象**: 目論見書、月次レポート

**方法**:
```python
import PyPDF2
import re

def extract_fund_info_from_pdf(pdf_path):
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
    
    # 正規表現で情報を抽出
    inception_date = re.search(r'設定日[：:]\s*(\d{4})年(\d{1,2})月(\d{1,2})日', text)
    expense_ratio = re.search(r'信託報酬[：:]\s*([\d.]+)%', text)
    
    return {
        'inception_date': inception_date.group(0) if inception_date else None,
        'expense_ratio': float(expense_ratio.group(1)) if expense_ratio else None
    }
```

### 3. データクレンジングと検証

**対象**: 収集したすべてのデータ

**方法**: 既存の`fund_performance_analysis.py`の`validate_and_clean_data()`メソッドを使用

---

## AI実行用ワークフロー定義

### フェーズ1: ファンドリストの作成

**入力**: なし

**処理**:
1. 投信総合検索ライブラリーにアクセス
2. 検索条件を設定:
   - 投資対象地域: 北米
   - 投資対象資産: 株式
   - 運用年数: 3年以上
3. 検索結果から以下の情報を抽出:
   - ファンド名
   - 運用会社
   - 基準価額
   - 純資産総額
4. ファンドリストをCSVに保存

**出力**: `data/fund_list_raw.csv`

**注意**: 投資信託協会のスクレイピング禁止規定により、手動収集が必要

### フェーズ2: ファンド属性の収集

**入力**: `data/fund_list_raw.csv`

**処理**:
1. 各ファンドの目論見書PDFをダウンロード（手動）
2. PDFから以下の情報を抽出:
   - 設定日
   - 信託報酬
   - ベンチマーク
   - 為替ヘッジの有無
   - 運用方針
3. アクティブ/パッシブの分類
4. テーマ型、レバレッジ型の除外

**出力**: `data/fund_attributes.csv`

### フェーズ3: 月次リターンの収集

**入力**: `data/fund_attributes.csv`

**処理**:
1. 各ファンドの月次基準価額を取得（直近48か月分）
   - 方法1: Yahoo!ファイナンスAPI
   - 方法2: 運用会社の月次レポート
   - 方法3: 手動入力
2. 月次リターンを計算
3. データの整合性チェック

**出力**: `data/monthly_returns.csv`

### フェーズ4: データ品質チェック

**入力**: 
- `data/fund_attributes.csv`
- `data/monthly_returns.csv`

**処理**:
1. `fund_performance_analysis.py`の`validate_and_clean_data()`を実行
2. 36か月要件のチェック
3. 異常値の検出
4. 除外ファンドリストの作成

**出力**:
- `output/excluded_funds_insufficient_data.csv`
- `output/outliers_detected.csv`

### フェーズ5: データの最終確認

**入力**: クレンジング済みデータ

**処理**:
1. ファンド数の確認
2. ヘッジあり/なしの分布確認
3. アクティブ/パッシブの分布確認
4. データ期間の確認

**出力**: データ収集レポート

---

## 実装スクリプト

### スクリプト1: Yahoo!ファイナンスからのデータ取得

**ファイル名**: `scripts/fetch_yahoo_finance_data.py`

**機能**:
- Yahoo!ファイナンスから投資信託の基準価額を取得
- 月次リターンを計算
- CSVに保存

**使用方法**:
```bash
python3 scripts/fetch_yahoo_finance_data.py --tickers ticker_list.txt --output data/monthly_returns_yahoo.csv
```

### スクリプト2: PDF情報抽出

**ファイル名**: `scripts/extract_fund_info_from_pdf.py`

**機能**:
- 目論見書PDFから基本情報を抽出
- 正規表現でパターンマッチング
- CSVに保存

**使用方法**:
```bash
python3 scripts/extract_fund_info_from_pdf.py --pdf-dir data/prospectus/ --output data/fund_attributes_extracted.csv
```

### スクリプト3: データ統合

**ファイル名**: `scripts/merge_fund_data.py`

**機能**:
- 複数のソースから収集したデータを統合
- 重複排除
- フォーマット統一

**使用方法**:
```bash
python3 scripts/merge_fund_data.py --sources data/fund_*_raw.csv --output data/fund_attributes.csv
```

---

## 現実的な実行計画

### 推奨アプローチ: ハイブリッド方式

**手動収集 + 自動処理の組み合わせ**

1. **手動作業（1～2日）**:
   - 投信総合検索ライブラリーで米国株式ファンドを検索
   - 検索結果からファンド名と基本情報をコピー
   - 代表的なファンド20～30本を選定

2. **半自動作業（0.5～1日）**:
   - 選定したファンドの目論見書PDFをダウンロード
   - PDFから情報を抽出（スクリプト使用）

3. **自動作業（数分）**:
   - Yahoo!ファイナンスから基準価額を取得（可能な場合）
   - 月次リターンを計算
   - データをフォーマット

4. **検証作業（0.5日）**:
   - データ品質チェック
   - 手動での補完・修正

**合計所要時間**: 2～4日

---

## 制約事項と限界

### データ収集の制約

1. **スクレイピング禁止**: 投資信託協会のサイトから自動収集不可
2. **データの網羅性**: 公開情報のみでは全ファンドを網羅できない可能性
3. **データ品質**: 手動収集の場合、入力ミスのリスク
4. **更新頻度**: 月次データの取得タイミングに制約

### 推奨される代替案

1. **有料データサービスの利用**:
   - モーニングスター Direct
   - Bloomberg
   - FactSet

2. **データベンダーとの契約**:
   - Quick
   - NEEDS

3. **サンプルデータでの分析**:
   - 代表的なファンドのみで分析を実施
   - 統計的な有意性は限定的だが、手法の検証には有効

---

## 次のステップ

1. データ収集方針の決定（手動 vs 有料サービス vs サンプル）
2. 選定したアプローチに基づくデータ収集の実施
3. 収集したデータの品質チェック
4. 分析フレームワークの実行

---

**作成者**: Manus AI
